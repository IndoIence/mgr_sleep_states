{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from model import ECGDataset, ECG_Classifier_LSTM\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from MyEDFImports import load_all_data, load_all_labels, remove_ecg_artifacts, three_stages_transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tempfile import TemporaryDirectory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T10:25:56.962144578Z",
     "start_time": "2023-08-10T10:25:55.525068653Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/CP229110.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/MyEDFImports.py:39: RuntimeWarning: Channel names are not unique, found duplicates for: {'CHIN EMG'}. Applying running numbers for duplicates.\n",
      "  raw = mne.io.read_raw_edf(path + \"//\" + name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/WD224010.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/TK221110.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/VP214110.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/CN223100.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/MyEDFImports.py:39: RuntimeWarning: Channel names are not unique, found duplicates for: {'CHIN EMG'}. Applying running numbers for duplicates.\n",
      "  raw = mne.io.read_raw_edf(path + \"//\" + name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/LM230010.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/VC209100.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/LA216100.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/DG220020.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/DO223050.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Extracting EDF parameters from /home/tadeusz/Desktop/Tadeusz/mgr_sleep_states/Jean-Pol_repaired_headers/CX230050.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "file with 2007 windows\n",
      "file with 1777 windows\n",
      "file with 1599 windows\n",
      "file with 1725 windows\n",
      "file with 1561 windows\n",
      "file with 1724 windows\n",
      "file with 1843 windows\n",
      "file with 1633 windows\n",
      "file with 1775 windows\n",
      "file with 1806 windows\n",
      "file with 1798 windows\n",
      "19248\n",
      "13604\n"
     ]
    }
   ],
   "source": [
    "all_unprepared_data = load_all_data()\n",
    "all_unprepared_labels = load_all_labels()\n",
    "\n",
    "print(len(all_unprepared_data))\n",
    "filtered_data, filter_labels = remove_ecg_artifacts(all_unprepared_data, all_unprepared_labels)\n",
    "print(len(filtered_data))\n",
    "# going from 6 labels to three Wake, Nrem, REM\n",
    "filter_labels = three_stages_transform(filter_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T10:26:04.969929241Z",
     "start_time": "2023-08-10T10:25:56.962798707Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 64  # Adjust based on the output from your conv layers\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3  # [Wake, NonREM, REM]\n",
    "learning_rate = 0.001\n",
    "batch_size=4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T10:26:04.970138140Z",
     "start_time": "2023-08-10T10:26:04.967650662Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "stages = ['train', 'val']\n",
    "dataset_all = ECGDataset(filtered_data, filter_labels)\n",
    "test_data, train_data = random_split(dataset_all, [0.8, 0.2])\n",
    "datasets = {'train': train_data, 'val': test_data}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in\n",
    "               stages}\n",
    "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ECG_Classifier_LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "model = model.to(device)  # Move the model to GPU"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T10:26:06.599553066Z",
     "start_time": "2023-08-10T10:26:04.972249309Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_name = f'best_model_params_{type(criterion).__name__}_{type(optimizer).__name__}_{num_epochs}.pt'\n",
    "        best_model_params_path = os.path.join(tempdir, best_model_name)\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # No idea why now for inputs why I need to transfer it to a float from a double\n",
    "                inputs = inputs.unsqueeze(1).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                torch.save(model.state_dict(), best_model_params_path)\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T10:43:51.025259881Z",
     "start_time": "2023-08-10T10:43:50.980850458Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# inbalanced dataset 4 to 1 so adding weights to criterion\n",
    "crit_weitghts = torch.tensor([4., 1., 4.]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=crit_weitghts)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model = ECG_Classifier_LSTM(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes, num_layers=num_layers)\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T10:48:25.030716641Z",
     "start_time": "2023-08-10T10:48:24.916672889Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 64, got 156",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexp_lr_scheduler\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[22], line 35\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# forward\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# track history if only in train\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(phase \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m---> 35\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m     _, preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     37\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Tadeusz/mgr_sleep_states/MY_CNN_RNN/model.py:31\u001B[0m, in \u001B[0;36mECG_Classifier_LSTM.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     29\u001B[0m h0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm\u001B[38;5;241m.\u001B[39mnum_layers, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm\u001B[38;5;241m.\u001B[39mhidden_size)\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     30\u001B[0m c0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm\u001B[38;5;241m.\u001B[39mnum_layers, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm\u001B[38;5;241m.\u001B[39mhidden_size)\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 31\u001B[0m x, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mh0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc0\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Pass through FC layer\u001B[39;00m\n\u001B[1;32m     34\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(x[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :])\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/rnn.py:810\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    806\u001B[0m     \u001B[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001B[39;00m\n\u001B[1;32m    807\u001B[0m     \u001B[38;5;66;03m# the user believes he/she is passing in.\u001B[39;00m\n\u001B[1;32m    808\u001B[0m     hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m--> 810\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_forward_args\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    812\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[1;32m    813\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/rnn.py:730\u001B[0m, in \u001B[0;36mLSTM.check_forward_args\u001B[0;34m(self, input, hidden, batch_sizes)\u001B[0m\n\u001B[1;32m    725\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_forward_args\u001B[39m(\u001B[38;5;28mself\u001B[39m,  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[1;32m    726\u001B[0m                        \u001B[38;5;28minput\u001B[39m: Tensor,\n\u001B[1;32m    727\u001B[0m                        hidden: Tuple[Tensor, Tensor],\n\u001B[1;32m    728\u001B[0m                        batch_sizes: Optional[Tensor],\n\u001B[1;32m    729\u001B[0m                        ):\n\u001B[0;32m--> 730\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_hidden_size(hidden[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_expected_hidden_size(\u001B[38;5;28minput\u001B[39m, batch_sizes),\n\u001B[1;32m    732\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected hidden[0] size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    733\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_hidden_size(hidden[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_expected_cell_size(\u001B[38;5;28minput\u001B[39m, batch_sizes),\n\u001B[1;32m    734\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected hidden[1] size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/rnn.py:218\u001B[0m, in \u001B[0;36mRNNBase.check_input\u001B[0;34m(self, input, batch_sizes)\u001B[0m\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    215\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput must have \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m dimensions, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    216\u001B[0m             expected_input_dim, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()))\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m--> 218\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput.size(-1) must be equal to input_size. Expected \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    220\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: input.size(-1) must be equal to input_size. Expected 64, got 156"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T10:48:28.114055046Z",
     "start_time": "2023-08-10T10:48:26.820788224Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(dataloaders['val']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T10:32:57.400879079Z",
     "start_time": "2023-08-10T10:32:57.181986438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.0001, -0.0001, -0.0001,  ..., -0.0006, -0.0006, -0.0006])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.unsqueeze(1)[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T10:41:10.434493436Z",
     "start_time": "2023-08-10T10:41:10.422502338Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
